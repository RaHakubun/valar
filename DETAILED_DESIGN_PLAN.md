# åŸºäºå·¥ä½œæµæ£€ç´¢-é€‚é…-åˆæˆçš„ComfyUIç”Ÿæˆç³»ç»Ÿ - è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ

## ğŸ¯ ç³»ç»Ÿæ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ç”¨æˆ·éœ€æ±‚è¾“å…¥                            â”‚
â”‚    "ç”Ÿæˆä¸€ä¸ªç²˜åœŸé£æ ¼çš„äººç‰©è‚–åƒï¼Œå¹¶è¿›è¡Œ4å€è¶…åˆ†"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ0: å·¥ä½œæµåº“ (Existing Workflows Library)             â”‚
â”‚  â€¢ å­˜å‚¨ç»“æ„                                               â”‚
â”‚  â€¢ æ„å›¾æ ‡æ³¨                                               â”‚
â”‚  â€¢ å‘é‡ç´¢å¼•                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: éœ€æ±‚åŒ¹é…                                          â”‚
â”‚  1.1 éœ€æ±‚åˆ†è§£ â†’ åŸå­éœ€æ±‚åˆ—è¡¨                              â”‚
â”‚  1.2 æ„å›¾åŒ¹é… â†’ Candidate Workflows                       â”‚
â”‚  1.3 éœ€æ±‚å¯¹é½ â†’ åŒ¹é…/æœªåŒ¹é…æ ‡è®°                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: å·¥ä½œæµæ¡†æ¶çº§åˆ«æ™ºèƒ½é€‚é…ï¼ˆä»£ç ç‰‡æ®µå½¢å¼ï¼‰            â”‚
â”‚  2.1 æ‹†åˆ†å·¥ä½œæµ â†’ åŸå­éœ€æ±‚æ˜ å°„ â†’ é‡ç»„æ‹¼æ¥                â”‚
â”‚  2.2 ç¼ºå¤±éƒ¨åˆ†ç”Ÿæˆ â†’ èŠ‚ç‚¹åŒ¹é… â†’ ä»£ç ç‰‡æ®µåˆæˆ              â”‚
â”‚  2.3 å®Œæ•´æ€§æ£€æŸ¥ â†’ è¯­ä¹‰æ£€æŸ¥ + è¯­æ³•æ£€æŸ¥                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: å¯æ‰§è¡Œå·¥ä½œæµåˆæˆ                                  â”‚
â”‚  3.1 å·¥ä½œæµæ¡†æ¶(Code) â†’ JSONè½¬æ¢                         â”‚
â”‚  3.2 å‚æ•°è¡¥å…¨                                             â”‚
â”‚  3.3 æ‰§è¡ŒéªŒè¯                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š é˜¶æ®µ0: å·¥ä½œæµåº“ï¼ˆExisting Workflows Libraryï¼‰

### 0.1 æ•°æ®ç»“æ„è®¾è®¡

#### æ ¸å¿ƒæ•°æ®æ¨¡å‹

```python
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

class WorkflowComplexity(Enum):
    VANILLA = "vanilla"      # ç®€å•ï¼Œå•ä¸€åŠŸèƒ½
    COMPLEX = "complex"      # å¤æ‚ï¼Œå¤šåŠŸèƒ½ç»„åˆ
    CREATIVE = "creative"    # åˆ›æ–°ï¼Œéœ€è¦ç†è§£åŸç†

@dataclass
class AtomicCapability:
    """åŸå­èƒ½åŠ›ï¼šå·¥ä½œæµä¸­ä¸å¯å†åˆ†çš„åŠŸèƒ½å•å…ƒ"""
    capability_id: str              # å¦‚ "text_encoding_clip"
    description: str                # "ä½¿ç”¨CLIPæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç¼–ç "
    category: str                   # "encoding", "generation", "upscaling"...
    
    # ä»£ç è¡¨ç¤º
    code_snippet: str               # Python-likeä»£ç ç‰‡æ®µ
    
    # èŠ‚ç‚¹ä¿¡æ¯
    node_ids: List[str]             # æ¶‰åŠçš„èŠ‚ç‚¹ID
    node_types: List[str]           # èŠ‚ç‚¹ç±»å‹åˆ—è¡¨
    
    # è¾“å…¥è¾“å‡ºåè®®
    inputs: Dict[str, str]          # {"clip": "CLIP", "text": "STRING"}
    outputs: Dict[str, str]         # {"conditioning": "CONDITIONING"}
    
    # ä¾èµ–å…³ç³»
    depends_on: List[str]           # ä¾èµ–çš„å…¶ä»–capability_id

@dataclass
class WorkflowIntent:
    """å·¥ä½œæµæ„å›¾"""
    primary_task: str               # "text-to-image"
    modality: str                   # "image", "video", "3d"
    style: Optional[str]            # "clay", "anime", "realistic"
    operation: str                  # "generation", "editing", "upscaling"
    
    # è‡ªç„¶è¯­è¨€æè¿°
    description: str                # "ç”Ÿæˆç²˜åœŸé£æ ¼çš„äººç‰©è‚–åƒ"
    keywords: List[str]             # ["ç²˜åœŸ", "äººç‰©", "è‚–åƒ", "ç”Ÿæˆ"]
    
    # åŸå­èƒ½åŠ›ç»„åˆ
    required_capabilities: List[str]  # å¿…éœ€çš„èƒ½åŠ›
    optional_capabilities: List[str]  # å¯é€‰çš„èƒ½åŠ›

@dataclass
class WorkflowEntry:
    """å·¥ä½œæµåº“ä¸­çš„ä¸€ä¸ªæ¡ç›®"""
    workflow_id: str
    
    # ä¸‰ç§è¡¨ç¤º
    workflow_json: Dict[str, Any]   # åŸå§‹JSON
    workflow_code: str              # Python-likeä»£ç 
    workflow_dag: Dict[str, Any]    # DAGåˆ†æç»“æœ
    
    # æ„å›¾å’Œèƒ½åŠ›
    intent: WorkflowIntent
    atomic_capabilities: List[AtomicCapability]
    
    # å…ƒæ•°æ®
    complexity: WorkflowComplexity
    node_count: int
    source: str                     # "comfybench", "openart", "manual"
    tags: List[str]
    
    # ç»Ÿè®¡ä¿¡æ¯
    usage_count: int
    success_rate: float
    avg_execution_time: float
    
    # å‘é‡è¡¨ç¤ºï¼ˆç”¨äºæ£€ç´¢ï¼‰
    intent_embedding: Optional[List[float]] = None
    capability_embedding: Optional[List[float]] = None
```

#### å­˜å‚¨ç»“æ„

```
data/
â”œâ”€â”€ workflow_library/
â”‚   â”œâ”€â”€ workflows/                    # å·¥ä½œæµJSON
â”‚   â”‚   â”œâ”€â”€ text_to_image_basic_001.json
â”‚   â”‚   â”œâ”€â”€ clay_style_portrait_002.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ metadata/                     # å…ƒæ•°æ®
â”‚   â”‚   â”œâ”€â”€ text_to_image_basic_001.meta.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ embeddings/                   # å‘é‡è¡¨ç¤º
â”‚   â”‚   â””â”€â”€ embeddings.faiss
â”‚   â”œâ”€â”€ index/                        # ç´¢å¼•
â”‚   â”‚   â”œâ”€â”€ intent_index.json         # æ„å›¾ç´¢å¼•
â”‚   â”‚   â”œâ”€â”€ capability_index.json     # èƒ½åŠ›ç´¢å¼•
â”‚   â”‚   â””â”€â”€ node_type_index.json      # èŠ‚ç‚¹ç±»å‹ç´¢å¼•
â”‚   â””â”€â”€ library.db                    # SQLiteæ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
```

### 0.2 å·¥ä½œæµé‡‡é›†å’Œæ ‡æ³¨

#### æ•°æ®æ¥æº

1. **ComfyBench Curriculum Workflows**ï¼ˆ20ä¸ªï¼‰
   - é«˜è´¨é‡ã€å·²éªŒè¯
   - è¦†ç›–åŸºç¡€åŠŸèƒ½

2. **ç¤¾åŒºå¹³å°çˆ¬å–**
   - OpenArt.aiï¼ˆè¿‡æ»¤é«˜è¯„åˆ†ï¼‰
   - ComfyWorkflows.com
   - Civitaiï¼ˆå¸¦å·¥ä½œæµçš„å›¾ç‰‡ï¼‰

3. **æ‰‹å·¥ç²¾é€‰**
   - ç‰¹å®šé¢†åŸŸçš„ä¼˜ç§€å·¥ä½œæµ

#### è‡ªåŠ¨æ ‡æ³¨æµç¨‹

```python
def auto_annotate_workflow(workflow_json: Dict) -> WorkflowEntry:
    """è‡ªåŠ¨æ ‡æ³¨å·¥ä½œæµ"""
    
    # 1. åŸºç¡€ä¿¡æ¯æå–
    dag_info = analyze_dag(workflow_json)
    code_repr = json_to_code(workflow_json)
    
    # 2. æ„å›¾æå–ï¼ˆä½¿ç”¨GPT-4ï¼‰
    intent_prompt = f"""
    åˆ†æä»¥ä¸‹ComfyUIå·¥ä½œæµï¼Œæå–å…¶æ„å›¾ï¼š
    
    å·¥ä½œæµèŠ‚ç‚¹ç±»å‹: {get_node_types(workflow_json)}
    å·¥ä½œæµç»“æ„: {dag_info['summary']}
    
    è¯·æå–ï¼š
    1. ä¸»è¦ä»»åŠ¡ï¼ˆå¦‚text-to-imageï¼‰
    2. æ¨¡æ€ï¼ˆimage/video/3dï¼‰
    3. é£æ ¼ï¼ˆå¦‚æœ‰ï¼‰
    4. æ“ä½œç±»å‹ï¼ˆgeneration/editing/upscalingï¼‰
    5. å…³é”®è¯åˆ—è¡¨
    6. è‡ªç„¶è¯­è¨€æè¿°
    """
    
    intent = gpt4_extract_intent(intent_prompt)
    
    # 3. åŸå­èƒ½åŠ›æå–ï¼ˆåŸºäºè§„åˆ™+LLMï¼‰
    capabilities = extract_atomic_capabilities(workflow_json, dag_info)
    
    # 4. å‘é‡åŒ–
    intent_embedding = embed_text(intent.description)
    capability_text = " ".join([c.description for c in capabilities])
    capability_embedding = embed_text(capability_text)
    
    return WorkflowEntry(
        workflow_id=generate_id(),
        workflow_json=workflow_json,
        workflow_code=code_repr,
        workflow_dag=dag_info,
        intent=intent,
        atomic_capabilities=capabilities,
        intent_embedding=intent_embedding,
        capability_embedding=capability_embedding,
        # ... å…¶ä»–å­—æ®µ
    )
```

#### åŸå­èƒ½åŠ›æå–ç®—æ³•

```python
def extract_atomic_capabilities(
    workflow_json: Dict,
    dag_info: Dict
) -> List[AtomicCapability]:
    """
    ä»å·¥ä½œæµä¸­æå–åŸå­èƒ½åŠ›
    
    ç­–ç•¥ï¼š
    1. åŸºäºèŠ‚ç‚¹ç±»å‹èšç±»ï¼ˆå¦‚æ‰€æœ‰CLIPç›¸å…³èŠ‚ç‚¹ï¼‰
    2. åŸºäºåŠŸèƒ½è¯­ä¹‰ï¼ˆå¦‚"æ–‡æœ¬ç¼–ç "ã€"é‡‡æ ·ç”Ÿæˆ"ï¼‰
    3. åŸºäºè¾“å…¥è¾“å‡ºè¾¹ç•Œ
    """
    
    capabilities = []
    
    # 1. è¯†åˆ«å…³é”®èŠ‚ç‚¹æ¨¡å¼
    patterns = {
        "text_encoding": ["CLIPTextEncode", "CLIPTextEncodeSDXL"],
        "image_generation": ["KSampler", "SamplerCustom"],
        "image_decoding": ["VAEDecode"],
        "upscaling": ["UpscaleModelLoader", "ImageUpscaleWithModel"],
        "controlnet": ["ControlNetLoader", "ControlNetApply"],
        # ... æ›´å¤šæ¨¡å¼
    }
    
    for pattern_name, node_types in patterns.items():
        matched_nodes = find_nodes_by_types(workflow_json, node_types)
        
        if matched_nodes:
            # æå–è¿™ä¸ªæ¨¡å¼çš„å®Œæ•´å­å›¾
            subgraph = extract_subgraph(
                workflow_json, 
                matched_nodes, 
                dag_info
            )
            
            # ç”Ÿæˆä»£ç ç‰‡æ®µ
            code_snippet = subgraph_to_code(subgraph)
            
            # åˆ†æè¾“å…¥è¾“å‡º
            inputs, outputs = analyze_io(subgraph, dag_info)
            
            capability = AtomicCapability(
                capability_id=f"{pattern_name}_{matched_nodes[0]}",
                description=get_pattern_description(pattern_name),
                category=get_pattern_category(pattern_name),
                code_snippet=code_snippet,
                node_ids=matched_nodes,
                node_types=node_types,
                inputs=inputs,
                outputs=outputs,
                depends_on=find_dependencies(subgraph, dag_info)
            )
            
            capabilities.append(capability)
    
    return capabilities
```

### 0.3 å·¥ä½œæµåº“ç®¡ç†

```python
class WorkflowLibrary:
    """å·¥ä½œæµåº“ç®¡ç†ç±»"""
    
    def __init__(self, library_path: str):
        self.library_path = library_path
        self.entries: Dict[str, WorkflowEntry] = {}
        self.vector_index = None  # FAISSç´¢å¼•
        
        self._load_library()
    
    def add_workflow(self, workflow_json: Dict, metadata: Dict = None):
        """æ·»åŠ æ–°å·¥ä½œæµ"""
        entry = auto_annotate_workflow(workflow_json)
        
        if metadata:
            entry.update(metadata)
        
        self.entries[entry.workflow_id] = entry
        self._update_index(entry)
        self._save_entry(entry)
    
    def search_by_intent(
        self, 
        query: str, 
        top_k: int = 10
    ) -> List[WorkflowEntry]:
        """åŸºäºæ„å›¾æ£€ç´¢å·¥ä½œæµ"""
        query_embedding = embed_text(query)
        
        # å‘é‡æ£€ç´¢
        distances, indices = self.vector_index.search(
            query_embedding.reshape(1, -1), 
            top_k * 2  # å¤šå¬å›ä¸€äº›å€™é€‰
        )
        
        candidates = [self.entries[self._id_map[idx]] for idx in indices[0]]
        
        # é‡æ’åº
        ranked = self._rerank(query, candidates, top_k)
        
        return ranked
    
    def search_by_capability(
        self,
        required_capabilities: List[str]
    ) -> List[WorkflowEntry]:
        """åŸºäºèƒ½åŠ›æ£€ç´¢å·¥ä½œæµ"""
        results = []
        
        for entry in self.entries.values():
            entry_caps = set(c.capability_id for c in entry.atomic_capabilities)
            required_caps = set(required_capabilities)
            
            # è®¡ç®—è¦†ç›–åº¦
            coverage = len(entry_caps & required_caps) / len(required_caps)
            
            if coverage > 0.5:  # è‡³å°‘è¦†ç›–50%
                results.append((coverage, entry))
        
        # æŒ‰è¦†ç›–åº¦æ’åº
        results.sort(key=lambda x: x[0], reverse=True)
        
        return [entry for _, entry in results]
    
    def _rerank(
        self, 
        query: str, 
        candidates: List[WorkflowEntry], 
        top_k: int
    ) -> List[WorkflowEntry]:
        """é‡æ’åºï¼šç»¼åˆè€ƒè™‘è¯­ä¹‰ã€åŠŸèƒ½è¦†ç›–åº¦ã€æˆåŠŸç‡ç­‰"""
        
        scores = []
        for candidate in candidates:
            # 1. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå·²åœ¨å‘é‡æ£€ç´¢ä¸­ä½“ç°ï¼‰
            semantic_score = 1.0  # å½’ä¸€åŒ–åçš„è·ç¦»
            
            # 2. å¤æ‚åº¦åŒ¹é…ï¼ˆç®€å•éœ€æ±‚ä¼˜å…ˆç®€å•å·¥ä½œæµï¼‰
            complexity_score = self._complexity_match(query, candidate)
            
            # 3. æˆåŠŸç‡
            success_score = candidate.success_rate
            
            # 4. ä½¿ç”¨é¢‘ç‡
            popularity_score = np.log1p(candidate.usage_count) / 10
            
            # ç»¼åˆå¾—åˆ†
            total_score = (
                0.5 * semantic_score +
                0.2 * complexity_score +
                0.2 * success_score +
                0.1 * popularity_score
            )
            
            scores.append((total_score, candidate))
        
        scores.sort(key=lambda x: x[0], reverse=True)
        return [candidate for _, candidate in scores[:top_k]]
```

---

## ğŸ¯ é˜¶æ®µ1: éœ€æ±‚åŒ¹é…

### 1.1 éœ€æ±‚åˆ†è§£

#### ç›®æ ‡
å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€éœ€æ±‚åˆ†è§£ä¸ºåŸå­éœ€æ±‚åˆ—è¡¨

#### è¾“å…¥è¾“å‡º

```python
@dataclass
class UserRequest:
    """ç”¨æˆ·éœ€æ±‚"""
    raw_text: str  # "ç”Ÿæˆä¸€ä¸ªç²˜åœŸé£æ ¼çš„äººç‰©è‚–åƒï¼Œå¹¶è¿›è¡Œ4å€è¶…åˆ†"
    context: Optional[Dict] = None  # å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

@dataclass
class AtomicNeed:
    """åŸå­éœ€æ±‚"""
    need_id: str
    description: str            # "ç”Ÿæˆç²˜åœŸé£æ ¼äººç‰©è‚–åƒ"
    category: str              # "generation", "upscaling", "editing"
    modality: str              # "image", "video", "3d"
    priority: int              # ä¼˜å…ˆçº§ 1-10
    dependencies: List[str]    # ä¾èµ–çš„å…¶ä»–need_id
    
    # çº¦æŸæ¡ä»¶
    constraints: Dict[str, Any]  # {"style": "clay", "subject": "portrait"}
    
    # å¯é€‰ï¼šæœŸæœ›çš„åŸå­èƒ½åŠ›
    expected_capabilities: List[str]

@dataclass
class DecomposedNeeds:
    """åˆ†è§£åçš„éœ€æ±‚"""
    atomic_needs: List[AtomicNeed]
    dependency_graph: Dict[str, List[str]]  # DAGç»“æ„
    execution_order: List[str]              # æ‹“æ‰‘æ’åºåçš„æ‰§è¡Œé¡ºåº
```

#### å®ç°ç®—æ³•

```python
class NeedDecomposer:
    """éœ€æ±‚åˆ†è§£å™¨"""
    
    def __init__(self, llm_client):
        self.llm = llm_client
        self.decomposition_prompt = self._load_prompt()
    
    def decompose(self, request: UserRequest) -> DecomposedNeeds:
        """åˆ†è§£éœ€æ±‚"""
        
        # 1. ä½¿ç”¨LLMè¿›è¡Œåˆæ­¥åˆ†è§£
        prompt = self._build_decomposition_prompt(request.raw_text)
        llm_output = self.llm.generate(prompt)
        
        # 2. è§£æLLMè¾“å‡º
        atomic_needs = self._parse_llm_output(llm_output)
        
        # 3. æ„å»ºä¾èµ–å›¾
        dependency_graph = self._build_dependency_graph(atomic_needs)
        
        # 4. æ‹“æ‰‘æ’åºï¼ˆç¡®å®šæ‰§è¡Œé¡ºåºï¼‰
        execution_order = self._topological_sort(dependency_graph)
        
        # 5. éªŒè¯å’Œä¼˜åŒ–
        atomic_needs = self._validate_and_optimize(atomic_needs)
        
        return DecomposedNeeds(
            atomic_needs=atomic_needs,
            dependency_graph=dependency_graph,
            execution_order=execution_order
        )
    
    def _build_decomposition_prompt(self, raw_text: str) -> str:
        """æ„å»ºåˆ†è§£æç¤ºè¯"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªComfyUIå·¥ä½œæµä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·éœ€æ±‚åˆ†è§£ä¸ºåŸå­éœ€æ±‚ã€‚

ç”¨æˆ·éœ€æ±‚: {raw_text}

è¯·åˆ†è§£ä¸ºåŸå­éœ€æ±‚ï¼Œæ¯ä¸ªåŸå­éœ€æ±‚åº”è¯¥ï¼š
1. æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€ä¸å¯å†åˆ†çš„åŠŸèƒ½å•å…ƒ
2. æ˜ç¡®æè¿°è¦åšä»€ä¹ˆ
3. æŒ‡å®šç±»åˆ«ï¼ˆgeneration/editing/upscalingç­‰ï¼‰
4. æŒ‡å®šæ¨¡æ€ï¼ˆimage/video/3dï¼‰
5. æŒ‡å®šä¾èµ–å…³ç³»ï¼ˆå¦‚æœéœ€è¦å‰ä¸€ä¸ªéœ€æ±‚çš„è¾“å‡ºï¼‰

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
    "atomic_needs": [
        {{
            "need_id": "need_1",
            "description": "ç”Ÿæˆç²˜åœŸé£æ ¼çš„äººç‰©è‚–åƒ",
            "category": "generation",
            "modality": "image",
            "priority": 10,
            "dependencies": [],
            "constraints": {{"style": "clay", "subject": "portrait"}}
        }},
        {{
            "need_id": "need_2",
            "description": "å°†å›¾åƒè¿›è¡Œ4å€è¶…åˆ†è¾¨ç‡å¤„ç†",
            "category": "upscaling",
            "modality": "image",
            "priority": 5,
            "dependencies": ["need_1"],
            "constraints": {{"scale": 4}}
        }}
    ]
}}
"""
    
    def _build_dependency_graph(
        self, 
        atomic_needs: List[AtomicNeed]
    ) -> Dict[str, List[str]]:
        """æ„å»ºä¾èµ–å›¾"""
        graph = {need.need_id: [] for need in atomic_needs}
        
        for need in atomic_needs:
            for dep in need.dependencies:
                graph[dep].append(need.need_id)
        
        return graph
    
    def _topological_sort(self, graph: Dict[str, List[str]]) -> List[str]:
        """æ‹“æ‰‘æ’åº"""
        # è®¡ç®—å…¥åº¦
        in_degree = {node: 0 for node in graph}
        for node in graph:
            for neighbor in graph[node]:
                in_degree[neighbor] += 1
        
        # Kahnç®—æ³•
        queue = [node for node in graph if in_degree[node] == 0]
        result = []
        
        while queue:
            node = queue.pop(0)
            result.append(node)
            
            for neighbor in graph[node]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)
        
        if len(result) != len(graph):
            raise ValueError("å­˜åœ¨å¾ªç¯ä¾èµ–ï¼")
        
        return result
```

### 1.2 æ„å›¾åŒ¹é… â†’ Candidate Workflows

#### ç›®æ ‡
å°†EWFï¼ˆå·¥ä½œæµåº“ï¼‰ä¸­çš„å·¥ä½œæµæ„å›¾åŒ¹é…åˆ°åŸå­éœ€æ±‚ä¸Š

#### å®ç°ç®—æ³•

```python
class IntentMatcher:
    """æ„å›¾åŒ¹é…å™¨"""
    
    def __init__(self, workflow_library: WorkflowLibrary):
        self.library = workflow_library
    
    def match(
        self, 
        atomic_needs: List[AtomicNeed],
        top_k_per_need: int = 5
    ) -> Dict[str, List[WorkflowEntry]]:
        """
        ä¸ºæ¯ä¸ªåŸå­éœ€æ±‚åŒ¹é…å€™é€‰å·¥ä½œæµ
        
        è¿”å›: {need_id: [candidate_workflow_1, ...]}
        """
        
        candidates_per_need = {}
        
        for need in atomic_needs:
            # æ„å»ºæŸ¥è¯¢
            query = self._build_query_from_need(need)
            
            # 1. åŸºäºæ„å›¾æ£€ç´¢ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰
            intent_matches = self.library.search_by_intent(
                query, 
                top_k=top_k_per_need * 2
            )
            
            # 2. åŸºäºèƒ½åŠ›æ£€ç´¢ï¼ˆåŠŸèƒ½åŒ¹é…ï¼‰
            if need.expected_capabilities:
                capability_matches = self.library.search_by_capability(
                    need.expected_capabilities
                )
            else:
                capability_matches = []
            
            # 3. åˆå¹¶å’Œå»é‡
            all_candidates = self._merge_candidates(
                intent_matches, 
                capability_matches
            )
            
            # 4. ç²¾ç»†åŒ–ç­›é€‰
            filtered = self._filter_by_constraints(all_candidates, need)
            
            # 5. é‡æ’åº
            ranked = self._rank_candidates(filtered, need, top_k_per_need)
            
            candidates_per_need[need.need_id] = ranked
        
        return candidates_per_need
    
    def _build_query_from_need(self, need: AtomicNeed) -> str:
        """ä»åŸå­éœ€æ±‚æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²"""
        query_parts = [need.description]
        
        # æ·»åŠ çº¦æŸä¿¡æ¯
        if need.constraints:
            for key, value in need.constraints.items():
                query_parts.append(f"{key}: {value}")
        
        return " ".join(query_parts)
    
    def _filter_by_constraints(
        self,
        candidates: List[WorkflowEntry],
        need: AtomicNeed
    ) -> List[WorkflowEntry]:
        """åŸºäºçº¦æŸæ¡ä»¶è¿‡æ»¤å€™é€‰å·¥ä½œæµ"""
        
        filtered = []
        for candidate in candidates:
            # æ£€æŸ¥æ¨¡æ€åŒ¹é…
            if candidate.intent.modality != need.modality:
                continue
            
            # æ£€æŸ¥ç±»åˆ«åŒ¹é…
            if candidate.intent.operation != need.category:
                continue
            
            # æ£€æŸ¥çº¦æŸæ¡ä»¶ï¼ˆå¦‚é£æ ¼ï¼‰
            if "style" in need.constraints:
                if candidate.intent.style != need.constraints["style"]:
                    # å…è®¸ä¸€å®šçš„çµæ´»æ€§
                    if not self._style_compatible(
                        candidate.intent.style, 
                        need.constraints["style"]
                    ):
                        continue
            
            filtered.append(candidate)
        
        return filtered
    
    def _rank_candidates(
        self,
        candidates: List[WorkflowEntry],
        need: AtomicNeed,
        top_k: int
    ) -> List[WorkflowEntry]:
        """é‡æ’åºå€™é€‰å·¥ä½œæµ"""
        
        scores = []
        for candidate in candidates:
            # ç»¼åˆå¾—åˆ†
            score = self._calculate_match_score(candidate, need)
            scores.append((score, candidate))
        
        scores.sort(key=lambda x: x[0], reverse=True)
        return [c for _, c in scores[:top_k]]
    
    def _calculate_match_score(
        self,
        candidate: WorkflowEntry,
        need: AtomicNeed
    ) -> float:
        """è®¡ç®—åŒ¹é…å¾—åˆ†"""
        
        score = 0.0
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        if candidate.intent_embedding:
            need_embedding = embed_text(need.description)
            semantic_sim = cosine_similarity(
                need_embedding,
                candidate.intent_embedding
            )
            score += 0.4 * semantic_sim
        
        # 2. èƒ½åŠ›è¦†ç›–åº¦
        if need.expected_capabilities:
            candidate_caps = set(
                c.capability_id for c in candidate.atomic_capabilities
            )
            expected_caps = set(need.expected_capabilities)
            coverage = len(candidate_caps & expected_caps) / len(expected_caps)
            score += 0.3 * coverage
        
        # 3. å¤æ‚åº¦åŒ¹é…ï¼ˆä¼˜å…ˆç®€å•çš„ï¼‰
        complexity_penalty = {
            WorkflowComplexity.VANILLA: 0,
            WorkflowComplexity.COMPLEX: -0.1,
            WorkflowComplexity.CREATIVE: -0.2
        }
        score += complexity_penalty.get(candidate.complexity, 0)
        
        # 4. æˆåŠŸç‡
        score += 0.2 * candidate.success_rate
        
        # 5. èŠ‚ç‚¹æ•°é‡ï¼ˆè¶Šå°‘è¶Šå¥½ï¼Œä½†ä¸èƒ½å¤ªå°‘ï¼‰
        node_score = 1.0 / (1.0 + abs(candidate.node_count - 10) / 10)
        score += 0.1 * node_score
        
        return score
```

### 1.3 éœ€æ±‚åŒ¹é… â†’ ç¡®å®šåŒ¹é…/æœªåŒ¹é…

#### ç›®æ ‡
åˆæ­¥ç¡®å®šå“ªäº›åŸå­éœ€æ±‚èƒ½åŒ¹é…ä¸Šå·¥ä½œæµï¼Œå“ªäº›åŒ¹é…ä¸ä¸Š

#### å®ç°ç®—æ³•

```python
@dataclass
class MatchResult:
    """åŒ¹é…ç»“æœ"""
    need_id: str
    matched: bool
    confidence: float           # åŒ¹é…ç½®ä¿¡åº¦ 0-1
    
    # å¦‚æœåŒ¹é…ä¸Š
    selected_workflow: Optional[WorkflowEntry] = None
    alternative_workflows: List[WorkflowEntry] = None
    
    # å¦‚æœæ²¡åŒ¹é…ä¸Š
    reason: Optional[str] = None
    missing_capabilities: List[str] = None

class NeedWorkflowMatcher:
    """éœ€æ±‚-å·¥ä½œæµåŒ¹é…å™¨"""
    
    def __init__(
        self,
        workflow_library: WorkflowLibrary,
        match_threshold: float = 0.6
    ):
        self.library = workflow_library
        self.match_threshold = match_threshold
        self.intent_matcher = IntentMatcher(workflow_library)
    
    def match_all(
        self,
        atomic_needs: List[AtomicNeed]
    ) -> List[MatchResult]:
        """ä¸ºæ‰€æœ‰åŸå­éœ€æ±‚æ‰¾åŒ¹é…"""
        
        # 1. æ‰¹é‡è·å–å€™é€‰å·¥ä½œæµ
        candidates_per_need = self.intent_matcher.match(
            atomic_needs,
            top_k_per_need=5
        )
        
        # 2. å¯¹æ¯ä¸ªéœ€æ±‚åšåŒ¹é…å†³ç­–
        results = []
        for need in atomic_needs:
            candidates = candidates_per_need.get(need.need_id, [])
            match_result = self._decide_match(need, candidates)
            results.append(match_result)
        
        return results
    
    def _decide_match(
        self,
        need: AtomicNeed,
        candidates: List[WorkflowEntry]
    ) -> MatchResult:
        """å†³ç­–æ˜¯å¦åŒ¹é…"""
        
        if not candidates:
            return MatchResult(
                need_id=need.need_id,
                matched=False,
                confidence=0.0,
                reason="æ²¡æœ‰æ‰¾åˆ°å€™é€‰å·¥ä½œæµ",
                missing_capabilities=need.expected_capabilities
            )
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰
        best_candidate = candidates[0]
        best_score = self.intent_matcher._calculate_match_score(
            best_candidate, need
        )
        
        # åˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if best_score >= self.match_threshold:
            return MatchResult(
                need_id=need.need_id,
                matched=True,
                confidence=best_score,
                selected_workflow=best_candidate,
                alternative_workflows=candidates[1:]
            )
        else:
            # åˆ†æä¸ºä»€ä¹ˆæ²¡åŒ¹é…ä¸Š
            missing_caps = self._analyze_missing_capabilities(
                need, best_candidate
            )
            
            return MatchResult(
                need_id=need.need_id,
                matched=False,
                confidence=best_score,
                reason=f"æœ€ä½³åŒ¹é…å¾—åˆ†{best_score:.2f}ä½äºé˜ˆå€¼{self.match_threshold}",
                missing_capabilities=missing_caps
            )
    
    def _analyze_missing_capabilities(
        self,
        need: AtomicNeed,
        best_candidate: WorkflowEntry
    ) -> List[str]:
        """åˆ†æç¼ºå¤±çš„èƒ½åŠ›"""
        
        if not need.expected_capabilities:
            return []
        
        candidate_caps = set(
            c.capability_id for c in best_candidate.atomic_capabilities
        )
        expected_caps = set(need.expected_capabilities)
        
        missing = list(expected_caps - candidate_caps)
        return missing
```

---

## âš™ï¸ é˜¶æ®µ2: å·¥ä½œæµæ¡†æ¶çº§åˆ«æ™ºèƒ½é€‚é…

### 2.1 æ‹†åˆ†åŸæœ‰å·¥ä½œæµ â†’ åŸå­éœ€æ±‚æ˜ å°„ â†’ é‡ç»„æ‹¼æ¥

#### ç›®æ ‡
å°†åŒ¹é…ä¸Šçš„å·¥ä½œæµæ‹†åˆ†ä¸ºç‰‡æ®µï¼Œä¸åŸå­éœ€æ±‚å¯¹åº”ï¼Œç„¶åé‡ç»„æ‹¼æ¥

#### æ ¸å¿ƒæ•°æ®ç»“æ„

```python
@dataclass
class WorkflowFragment:
    """å·¥ä½œæµç‰‡æ®µ"""
    fragment_id: str
    source_workflow_id: str
    
    # ä»£ç è¡¨ç¤º
    code_snippet: str
    
    # èŠ‚ç‚¹ä¿¡æ¯
    node_ids: List[str]         # åŸå§‹èŠ‚ç‚¹ID
    node_json: Dict[str, Any]   # èŠ‚ç‚¹JSON
    
    # æ˜ å°„åˆ°çš„åŸå­éœ€æ±‚
    mapped_need_id: Optional[str] = None
    
    # è¾“å…¥è¾“å‡º
    inputs: Dict[str, str]      # {"image": "IMAGE", "text": "STRING"}
    outputs: Dict[str, str]     # {"result": "IMAGE"}
    
    # DAGä¿¡æ¯
    internal_dependencies: Dict[str, List[str]]  # ç‰‡æ®µå†…éƒ¨ä¾èµ–
    external_dependencies: List[str]             # ä¾èµ–å…¶ä»–ç‰‡æ®µ

@dataclass
class WorkflowFramework:
    """å·¥ä½œæµæ¡†æ¶ï¼ˆä»£ç ç‰‡æ®µç»„åˆï¼‰"""
    framework_id: str
    fragments: List[WorkflowFragment]
    execution_order: List[str]  # fragment_idçš„æ‰§è¡Œé¡ºåº
    
    # ä»£ç è¡¨ç¤º
    framework_code: str
```

#### å®ç°ç®—æ³•

```python
class WorkflowAdapter:
    """å·¥ä½œæµé€‚é…å™¨"""
    
    def __init__(self):
        # ä»å‰ä½œä¸­æå–çš„ç®—æ³•
        self.fragment_splitter = FragmentSplitter()
        self.fragment_assembler = FragmentAssembler()
        self.validator = WorkflowValidator()
    
    def adapt(
        self,
        atomic_needs: List[AtomicNeed],
        match_results: List[MatchResult]
    ) -> WorkflowFramework:
        """
        æ™ºèƒ½é€‚é…ï¼šæ‹†åˆ†-æ˜ å°„-æ‹¼æ¥
        """
        
        # 1. æ‹†åˆ†æ‰€æœ‰åŒ¹é…ä¸Šçš„å·¥ä½œæµ
        all_fragments = []
        for match in match_results:
            if match.matched:
                fragments = self.fragment_splitter.split(
                    match.selected_workflow
                )
                all_fragments.extend(fragments)
        
        # 2. å°†ç‰‡æ®µæ˜ å°„åˆ°åŸå­éœ€æ±‚
        mapped_fragments = self._map_fragments_to_needs(
            all_fragments,
            atomic_needs
        )
        
        # 3. é€‰æ‹©æœ€ä¼˜ç‰‡æ®µç»„åˆ
        selected_fragments = self._select_optimal_fragments(
            mapped_fragments,
            atomic_needs
        )
        
        # 4. æ‹¼æ¥ç‰‡æ®µ
        framework = self.fragment_assembler.assemble(
            selected_fragments,
            atomic_needs
        )
        
        return framework
    
    def _map_fragments_to_needs(
        self,
        fragments: List[WorkflowFragment],
        atomic_needs: List[AtomicNeed]
    ) -> Dict[str, List[WorkflowFragment]]:
        """
        å°†ç‰‡æ®µæ˜ å°„åˆ°åŸå­éœ€æ±‚
        
        è¿”å›: {need_id: [fragment_1, fragment_2, ...]}
        """
        
        mapping = {need.need_id: [] for need in atomic_needs}
        
        for fragment in fragments:
            best_match = None
            best_score = 0
            
            for need in atomic_needs:
                # è®¡ç®—ç‰‡æ®µä¸éœ€æ±‚çš„ç›¸ä¼¼åº¦
                score = self._calculate_fragment_need_similarity(
                    fragment, need
                )
                
                if score > best_score:
                    best_score = score
                    best_match = need.need_id
            
            if best_match and best_score > 0.5:
                fragment.mapped_need_id = best_match
                mapping[best_match].append(fragment)
        
        return mapping
    
    def _select_optimal_fragments(
        self,
        mapped_fragments: Dict[str, List[WorkflowFragment]],
        atomic_needs: List[AtomicNeed]
    ) -> List[WorkflowFragment]:
        """
        ä¸ºæ¯ä¸ªåŸå­éœ€æ±‚é€‰æ‹©æœ€ä¼˜ç‰‡æ®µ
        """
        
        selected = []
        
        for need in atomic_needs:
            candidates = mapped_fragments.get(need.need_id, [])
            
            if not candidates:
                # æ²¡æœ‰åŒ¹é…çš„ç‰‡æ®µï¼Œæ ‡è®°ä¸ºéœ€è¦ç”Ÿæˆ
                selected.append(None)  # å ä½
            else:
                # é€‰æ‹©æœ€ä¼˜ç‰‡æ®µï¼ˆç»¼åˆè€ƒè™‘å¤æ‚åº¦ã€æˆåŠŸç‡ç­‰ï¼‰
                best = self._rank_fragments(candidates, need)[0]
                selected.append(best)
        
        return selected

class FragmentSplitter:
    """ç‰‡æ®µæ‹†åˆ†å™¨"""
    
    def split(self, workflow: WorkflowEntry) -> List[WorkflowFragment]:
        """
        å°†å·¥ä½œæµæ‹†åˆ†ä¸ºç‰‡æ®µ
        
        ç­–ç•¥ï¼š
        1. åŸºäºåŸå­èƒ½åŠ›æ‹†åˆ†ï¼ˆå·²æ ‡æ³¨ï¼‰
        2. åŸºäºåŠŸèƒ½è¾¹ç•Œæ‹†åˆ†
        """
        
        fragments = []
        
        # ä½¿ç”¨å·²æ ‡æ³¨çš„åŸå­èƒ½åŠ›
        for capability in workflow.atomic_capabilities:
            fragment = WorkflowFragment(
                fragment_id=f"{workflow.workflow_id}_{capability.capability_id}",
                source_workflow_id=workflow.workflow_id,
                code_snippet=capability.code_snippet,
                node_ids=capability.node_ids,
                node_json=self._extract_nodes(
                    workflow.workflow_json,
                    capability.node_ids
                ),
                inputs=capability.inputs,
                outputs=capability.outputs,
                internal_dependencies=self._build_internal_deps(
                    capability.node_ids,
                    workflow.workflow_dag
                )
            )
            
            fragments.append(fragment)
        
        return fragments

class FragmentAssembler:
    """ç‰‡æ®µç»„è£…å™¨ï¼ˆä½¿ç”¨å‰ä½œç®—æ³•ï¼‰"""
    
    def assemble(
        self,
        fragments: List[WorkflowFragment],
        atomic_needs: List[AtomicNeed]
    ) -> WorkflowFramework:
        """
        æ‹¼æ¥ç‰‡æ®µæˆå®Œæ•´å·¥ä½œæµæ¡†æ¶
        
        å…³é”®ï¼šä½¿ç”¨å‰ä½œçš„æ‹¼æ¥ç®—æ³•ï¼
        """
        
        # 1. ç¡®å®šæ‰§è¡Œé¡ºåºï¼ˆåŸºäºåŸå­éœ€æ±‚çš„ä¾èµ–å›¾ï¼‰
        execution_order = self._determine_execution_order(
            fragments, atomic_needs
        )
        
        # 2. æŒ‰é¡ºåºæ‹¼æ¥ï¼ˆä½¿ç”¨å‰ä½œç®—æ³•ï¼‰
        assembled_json = {}
        offset = 0
        
        for i, fragment in enumerate(execution_order):
            if fragment is None:
                # ç¼ºå¤±ç‰‡æ®µï¼Œè·³è¿‡ï¼ˆåç»­ç”Ÿæˆï¼‰
                continue
            
            # IDåç§»ï¼ˆå‰ä½œç®—æ³•ï¼‰
            fragment_json = update_node_numbers(
                fragment.node_json,
                offset
            )
            
            # åˆå¹¶
            assembled_json = merge_dicts_update(
                assembled_json,
                fragment_json
            )
            
            # å¤„ç†ä¾èµ–è¿æ¥
            if i > 0 and execution_order[i-1] is not None:
                assembled_json = self._connect_fragments(
                    assembled_json,
                    execution_order[i-1],
                    fragment,
                    offset
                )
            
            offset = find_max_id(assembled_json)  # å‰ä½œç®—æ³•
        
        # 3. è½¬æ¢ä¸ºä»£ç è¡¨ç¤º
        framework_code = json_to_code(assembled_json)
        
        return WorkflowFramework(
            framework_id=generate_id(),
            fragments=[f for f in execution_order if f],
            execution_order=[f.fragment_id for f in execution_order if f],
            framework_code=framework_code
        )
    
    def _connect_fragments(
        self,
        assembled_json: Dict,
        prev_fragment: WorkflowFragment,
        curr_fragment: WorkflowFragment,
        offset: int
    ) -> Dict:
        """
        è¿æ¥ä¸¤ä¸ªç‰‡æ®µï¼ˆå¤„ç†è¾“å…¥è¾“å‡ºï¼‰
        
        ä½¿ç”¨å‰ä½œçš„è¿æ¥é€»è¾‘ï¼š
        - æ‰¾åˆ°prevçš„è¾“å‡ºèŠ‚ç‚¹
        - æ‰¾åˆ°currçš„è¾“å…¥èŠ‚ç‚¹
        - ä¿®æ”¹currçš„è¾“å…¥è¿æ¥æŒ‡å‘prevçš„è¾“å‡º
        """
        
        # æ‰¾åˆ°prevçš„è¾“å‡º
        prev_output_types = set(prev_fragment.outputs.values())
        
        # æ‰¾åˆ°curréœ€è¦çš„è¾“å…¥
        curr_input_types = set(curr_fragment.inputs.values())
        
        # åŒ¹é…ç±»å‹
        for output_type in prev_output_types:
            if output_type in curr_input_types:
                # æ‰¾åˆ°å…·ä½“çš„è¾“å‡ºèŠ‚ç‚¹ID
                prev_output_node = self._find_output_node_by_type(
                    prev_fragment,
                    output_type
                )
                
                # æ‰¾åˆ°å…·ä½“çš„è¾“å…¥å­—æ®µ
                curr_input_field = self._find_input_field_by_type(
                    curr_fragment,
                    output_type
                )
                
                # ä¿®æ”¹è¿æ¥ï¼ˆå‰ä½œçš„modify_node_inputé€»è¾‘ï¼‰
                assembled_json = modify_node_input(
                    assembled_json,
                    curr_fragment.node_json[curr_fragment.node_ids[0]]['class_type'],
                    curr_input_field,
                    str(int(prev_output_node) + offset - find_max_id(prev_fragment.node_json))
                )
        
        return assembled_json
```

### 2.2 æœªåŒ¹é…éœ€æ±‚ç”Ÿæˆ â†’ èŠ‚ç‚¹åŒ¹é… â†’ ä»£ç ç‰‡æ®µåˆæˆ

#### ç›®æ ‡
å¯¹äºæ²¡æœ‰åŒ¹é…ä¸Šå·¥ä½œæµçš„åŸå­éœ€æ±‚ï¼Œé€šè¿‡èŠ‚ç‚¹åŒ¹é…ç”Ÿæˆä»£ç ç‰‡æ®µ

#### å®ç°ç®—æ³•

```python
class FragmentGenerator:
    """ç‰‡æ®µç”Ÿæˆå™¨ï¼ˆç”¨äºæœªåŒ¹é…çš„éœ€æ±‚ï¼‰"""
    
    def __init__(self, node_library: Dict):
        self.node_library = node_library  # èŠ‚ç‚¹çŸ¥è¯†åº“
        self.pattern_templates = self._load_pattern_templates()
    
    def generate_fragment(
        self,
        need: AtomicNeed
    ) -> WorkflowFragment:
        """
        ä¸ºæœªåŒ¹é…çš„åŸå­éœ€æ±‚ç”Ÿæˆç‰‡æ®µ
        
        ç­–ç•¥ï¼š
        1. åŸºäºéœ€æ±‚ç±»åˆ«åŒ¹é…æ¨¡æ¿
        2. åŸºäºèŠ‚ç‚¹åŠŸèƒ½æœç´¢
        3. ä½¿ç”¨LLMè¾…åŠ©ç”Ÿæˆ
        """
        
        # 1. å°è¯•åŒ¹é…å·²çŸ¥æ¨¡æ¿
        template = self._match_template(need)
        if template:
            return self._instantiate_template(template, need)
        
        # 2. åŸºäºèŠ‚ç‚¹åŠŸèƒ½æœç´¢
        relevant_nodes = self._search_nodes_by_function(need)
        if relevant_nodes:
            return self._compose_from_nodes(relevant_nodes, need)
        
        # 3. ä½¿ç”¨LLMç”Ÿæˆï¼ˆæœ€åæ‰‹æ®µï¼‰
        return self._llm_generate(need)
    
    def _match_template(self, need: AtomicNeed) -> Optional[Dict]:
        """åŒ¹é…å·²çŸ¥æ¨¡æ¿"""
        
        # ä¾‹å¦‚ï¼šè¶…åˆ†è¾¨ç‡æ¨¡æ¿
        if need.category == "upscaling":
            scale = need.constraints.get("scale", 2)
            return self.pattern_templates["upscaling"].get(f"{scale}x")
        
        # ä¾‹å¦‚ï¼šæ–‡ç”Ÿå›¾æ¨¡æ¿
        if need.category == "generation" and need.modality == "image":
            return self.pattern_templates["text_to_image"]["basic"]
        
        return None
    
    def _instantiate_template(
        self,
        template: Dict,
        need: AtomicNeed
    ) -> WorkflowFragment:
        """å®ä¾‹åŒ–æ¨¡æ¿"""
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„å‚æ•°
        code = template["code_template"]
        
        # æ›¿æ¢çº¦æŸæ¡ä»¶
        for key, value in need.constraints.items():
            placeholder = f"{{{{key}}}}"
            code = code.replace(placeholder, str(value))
        
        # ç”ŸæˆèŠ‚ç‚¹JSON
        node_json = code_to_json(code)
        
        return WorkflowFragment(
            fragment_id=f"generated_{need.need_id}",
            source_workflow_id="template",
            code_snippet=code,
            node_json=node_json,
            mapped_need_id=need.need_id,
            # ... å…¶ä»–å­—æ®µ
        )
    
    def _search_nodes_by_function(
        self,
        need: AtomicNeed
    ) -> List[str]:
        """åŸºäºåŠŸèƒ½æœç´¢ç›¸å…³èŠ‚ç‚¹"""
        
        # åœ¨èŠ‚ç‚¹åº“ä¸­æœç´¢
        query = f"{need.category} {need.modality}"
        
        relevant_nodes = []
        for node_name, node_info in self.node_library.items():
            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            similarity = semantic_similarity(
                query,
                node_info["description"]
            )
            
            if similarity > 0.7:
                relevant_nodes.append(node_name)
        
        return relevant_nodes
    
    def _compose_from_nodes(
        self,
        node_types: List[str],
        need: AtomicNeed
    ) -> WorkflowFragment:
        """ä»èŠ‚ç‚¹ç»„åˆç”Ÿæˆç‰‡æ®µ"""
        
        # ä½¿ç”¨å‰ä½œçš„æ¨¡å—å·¥ä½œæµæ„å»ºé€»è¾‘
        # å‚è€ƒyaml_to_workflow.pyçš„ç”Ÿæˆé€»è¾‘
        
        # 1. ç¡®å®šèŠ‚ç‚¹é¡ºåº
        ordered_nodes = self._determine_node_order(node_types, need)
        
        # 2. ç”Ÿæˆä»£ç 
        code = self._generate_code_from_nodes(ordered_nodes, need)
        
        # 3. è½¬æ¢ä¸ºJSON
        node_json = code_to_json(code)
        
        return WorkflowFragment(
            fragment_id=f"composed_{need.need_id}",
            source_workflow_id="composition",
            code_snippet=code,
            node_json=node_json,
            mapped_need_id=need.need_id,
            # ... å…¶ä»–å­—æ®µ
        )
    
    def _llm_generate(self, need: AtomicNeed) -> WorkflowFragment:
        """ä½¿ç”¨LLMç”Ÿæˆç‰‡æ®µï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
        
        prompt = f"""
ç”Ÿæˆä¸€ä¸ªComfyUIå·¥ä½œæµç‰‡æ®µæ¥å®ç°ä»¥ä¸‹éœ€æ±‚ï¼š

éœ€æ±‚æè¿°: {need.description}
ç±»åˆ«: {need.category}
æ¨¡æ€: {need.modality}
çº¦æŸ: {need.constraints}

è¯·ç”ŸæˆPythoné£æ ¼çš„ä»£ç è¡¨ç¤ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
model, clip, vae = CheckpointLoaderSimple(ckpt_name="model.safetensors")
conditioning = CLIPTextEncode(clip=clip, text="prompt")
...

åªè¿”å›ä»£ç ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
        
        llm_code = self.llm.generate(prompt)
        
        # éªŒè¯ç”Ÿæˆçš„ä»£ç 
        try:
            node_json = code_to_json(llm_code)
            
            return WorkflowFragment(
                fragment_id=f"llm_generated_{need.need_id}",
                source_workflow_id="llm",
                code_snippet=llm_code,
                node_json=node_json,
                mapped_need_id=need.need_id,
                # ... å…¶ä»–å­—æ®µ
            )
        except Exception as e:
            raise ValueError(f"LLMç”Ÿæˆçš„ä»£ç æ— æ•ˆ: {e}")
```

### 2.3 å®Œæ•´æ€§æ£€æŸ¥ â†’ è¯­ä¹‰æ£€æŸ¥ + è¯­æ³•æ£€æŸ¥

#### ç›®æ ‡
æ£€æŸ¥å·¥ä½œæµæ¡†æ¶æ˜¯å¦å®Œæ•´ã€æ­£ç¡®

#### å®ç°ç®—æ³•

```python
class WorkflowValidator:
    """å·¥ä½œæµéªŒè¯å™¨"""
    
    def validate(self, framework: WorkflowFramework) -> Tuple[bool, List[str]]:
        """
        éªŒè¯å·¥ä½œæµæ¡†æ¶
        
        è¿”å›: (is_valid, error_messages)
        """
        
        errors = []
        
        # 1. è¯­æ³•æ£€æŸ¥
        syntax_errors = self._check_syntax(framework)
        errors.extend(syntax_errors)
        
        # 2. è¯­ä¹‰æ£€æŸ¥
        semantic_errors = self._check_semantics(framework)
        errors.extend(semantic_errors)
        
        # 3. å®Œæ•´æ€§æ£€æŸ¥
        completeness_errors = self._check_completeness(framework)
        errors.extend(completeness_errors)
        
        return len(errors) == 0, errors
    
    def _check_syntax(self, framework: WorkflowFramework) -> List[str]:
        """
        è¯­æ³•æ£€æŸ¥
        
        æ£€æŸ¥é¡¹ï¼š
        1. DAGç»“æ„æœ‰æ•ˆï¼ˆæ— ç¯ï¼‰
        2. èŠ‚ç‚¹IDå”¯ä¸€
        3. èŠ‚ç‚¹ç±»å‹æœ‰æ•ˆ
        4. è¾“å…¥è¾“å‡ºç±»å‹åŒ¹é…
        """
        
        errors = []
        
        # å°†ä»£ç è½¬æ¢ä¸ºJSON
        try:
            workflow_json = code_to_json(framework.framework_code)
        except Exception as e:
            errors.append(f"ä»£ç è½¬JSONå¤±è´¥: {e}")
            return errors
        
        # 1. DAGæ£€æŸ¥ï¼ˆä½¿ç”¨å‰ä½œç®—æ³•ï¼‰
        if not is_valid_dag(workflow_json):
            errors.append("DAGç»“æ„æ— æ•ˆï¼ˆå­˜åœ¨ç¯æˆ–ä¸è¿é€šï¼‰")
        
        # 2. èŠ‚ç‚¹IDå”¯ä¸€æ€§
        node_ids = set()
        for node_id in workflow_json.keys():
            if node_id in node_ids:
                errors.append(f"é‡å¤çš„èŠ‚ç‚¹ID: {node_id}")
            node_ids.add(node_id)
        
        # 3. èŠ‚ç‚¹ç±»å‹æœ‰æ•ˆæ€§
        valid_node_types = set(self.node_library.keys())
        for node_id, node in workflow_json.items():
            if node['class_type'] not in valid_node_types:
                errors.append(f"æ— æ•ˆçš„èŠ‚ç‚¹ç±»å‹: {node['class_type']} (èŠ‚ç‚¹{node_id})")
        
        # 4. ç±»å‹åŒ¹é…æ£€æŸ¥
        type_errors = self._check_type_compatibility(workflow_json)
        errors.extend(type_errors)
        
        return errors
    
    def _check_type_compatibility(self, workflow_json: Dict) -> List[str]:
        """æ£€æŸ¥ç±»å‹å…¼å®¹æ€§"""
        
        errors = []
        
        for node_id, node in workflow_json.items():
            for input_name, input_value in node['inputs'].items():
                # å¦‚æœæ˜¯èŠ‚ç‚¹è¿æ¥
                if isinstance(input_value, list) and len(input_value) == 2:
                    source_node_id, output_slot = input_value
                    
                    # è·å–æºèŠ‚ç‚¹çš„è¾“å‡ºç±»å‹
                    source_node = workflow_json.get(str(source_node_id))
                    if not source_node:
                        errors.append(
                            f"èŠ‚ç‚¹{node_id}å¼•ç”¨äº†ä¸å­˜åœ¨çš„èŠ‚ç‚¹{source_node_id}"
                        )
                        continue
                    
                    source_output_type = self._get_output_type(
                        source_node,
                        output_slot
                    )
                    
                    # è·å–å½“å‰èŠ‚ç‚¹æœŸæœ›çš„è¾“å…¥ç±»å‹
                    expected_input_type = self._get_expected_input_type(
                        node,
                        input_name
                    )
                    
                    # æ£€æŸ¥å…¼å®¹æ€§
                    if not self._type_compatible(source_output_type, expected_input_type):
                        errors.append(
                            f"ç±»å‹ä¸åŒ¹é…: èŠ‚ç‚¹{source_node_id}çš„è¾“å‡ºç±»å‹"
                            f"{source_output_type}ä¸èŠ‚ç‚¹{node_id}çš„è¾“å…¥{input_name}"
                            f"æœŸæœ›ç±»å‹{expected_input_type}ä¸å…¼å®¹"
                        )
        
        return errors
    
    def _check_semantics(self, framework: WorkflowFramework) -> List[str]:
        """
        è¯­ä¹‰æ£€æŸ¥
        
        æ£€æŸ¥é¡¹ï¼š
        1. å·¥ä½œæµé€»è¾‘åˆç†æ€§
        2. å¿…è¦èŠ‚ç‚¹å­˜åœ¨ï¼ˆå¦‚SaveImageï¼‰
        3. å‚æ•°åˆç†æ€§
        """
        
        errors = []
        
        workflow_json = code_to_json(framework.framework_code)
        
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰è¾“å‡ºèŠ‚ç‚¹
        has_output = False
        output_node_types = ['SaveImage', 'SaveVideo', 'PreviewImage']
        
        for node in workflow_json.values():
            if node['class_type'] in output_node_types:
                has_output = True
                break
        
        if not has_output:
            errors.append("å·¥ä½œæµç¼ºå°‘è¾“å‡ºèŠ‚ç‚¹ï¼ˆSaveImage/SaveVideoç­‰ï¼‰")
        
        # 2. æ£€æŸ¥é€»è¾‘é“¾è·¯å®Œæ•´æ€§
        start_nodes, end_nodes = find_start_end_nodes(workflow_json)
        
        if not start_nodes:
            errors.append("å·¥ä½œæµæ²¡æœ‰èµ·å§‹èŠ‚ç‚¹")
        
        if not end_nodes:
            errors.append("å·¥ä½œæµæ²¡æœ‰ç»“æŸèŠ‚ç‚¹")
        
        # 3. æ£€æŸ¥å‚æ•°åˆç†æ€§
        param_errors = self._check_parameters(workflow_json)
        errors.extend(param_errors)
        
        return errors
    
    def _check_completeness(self, framework: WorkflowFramework) -> List[str]:
        """
        å®Œæ•´æ€§æ£€æŸ¥
        
        æ£€æŸ¥æ‰€æœ‰ç‰‡æ®µæ˜¯å¦æ­£ç¡®æ‹¼æ¥
        """
        
        errors = []
        
        # æ£€æŸ¥ç‰‡æ®µé—´çš„è¿æ¥
        for i in range(len(framework.fragments) - 1):
            curr_fragment = framework.fragments[i]
            next_fragment = framework.fragments[i + 1]
            
            # æ£€æŸ¥currçš„è¾“å‡ºæ˜¯å¦è¢«nextæ¶ˆè´¹
            curr_outputs = set(curr_fragment.outputs.values())
            next_inputs = set(next_fragment.inputs.values())
            
            # åº”è¯¥è‡³å°‘æœ‰ä¸€ä¸ªç±»å‹åŒ¹é…
            if not (curr_outputs & next_inputs):
                errors.append(
                    f"ç‰‡æ®µ{curr_fragment.fragment_id}å’Œ"
                    f"{next_fragment.fragment_id}ä¹‹é—´æ²¡æœ‰æ•°æ®æµè¿æ¥"
                )
        
        return errors
```

---

## ğŸ”¨ é˜¶æ®µ3: å¯æ‰§è¡Œå·¥ä½œæµåˆæˆ

### 3.1 å·¥ä½œæµæ¡†æ¶ â†’ JSONè½¬æ¢

#### ç›®æ ‡
å°†å·¥ä½œæµæ¡†æ¶ï¼ˆä»£ç è¡¨ç¤ºï¼‰è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„JSONæ ¼å¼

#### å®ç°ç®—æ³•

```python
class WorkflowSynthesizer:
    """å·¥ä½œæµåˆæˆå™¨"""
    
    def synthesize(
        self,
        framework: WorkflowFramework,
        user_request: UserRequest,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        åˆæˆå¯æ‰§è¡Œå·¥ä½œæµ
        
        è¾“å…¥ï¼š
        - framework: éªŒè¯é€šè¿‡çš„å·¥ä½œæµæ¡†æ¶
        - user_request: åŸå§‹ç”¨æˆ·éœ€æ±‚ï¼ˆç”¨äºå‚æ•°è¡¥å…¨ï¼‰
        - context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼‰
        
        è¾“å‡ºï¼š
        - å¯æ‰§è¡Œçš„JSONå·¥ä½œæµ
        """
        
        # 1. ä»£ç  â†’ JSONè½¬æ¢ï¼ˆä½¿ç”¨å·²æœ‰çš„åŒå‘è§£æå™¨ï¼‰
        workflow_json = code_to_json(framework.framework_code)
        
        # 2. å‚æ•°è¡¥å…¨å’Œä¼˜åŒ–
        workflow_json = self._complete_parameters(
            workflow_json,
            user_request,
            context
        )
        
        # 3. æœ€ç»ˆéªŒè¯
        is_valid, errors = self._final_validation(workflow_json)
        if not is_valid:
            raise ValueError(f"æœ€ç»ˆéªŒè¯å¤±è´¥: {errors}")
        
        return workflow_json
    
    def _complete_parameters(
        self,
        workflow_json: Dict,
        user_request: UserRequest,
        context: Optional[Dict]
    ) -> Dict:
        """
        å‚æ•°è¡¥å…¨
        
        è¡¥å…¨å†…å®¹ï¼š
        1. æç¤ºè¯ï¼ˆä»ç”¨æˆ·éœ€æ±‚ä¸­æå–ï¼‰
        2. æ–‡ä»¶è·¯å¾„ï¼ˆä»ä¸Šä¸‹æ–‡ä¸­è·å–ï¼‰
        3. é»˜è®¤å‚æ•°ï¼ˆseedã€æ­¥æ•°ç­‰ï¼‰
        """
        
        # 1. è¡¥å…¨æç¤ºè¯
        workflow_json = self._fill_prompts(workflow_json, user_request)
        
        # 2. è¡¥å…¨æ–‡ä»¶è·¯å¾„
        if context and "input_file" in context:
            workflow_json = self._fill_input_paths(
                workflow_json,
                context["input_file"]
            )
        
        # 3. è¡¥å…¨è¾“å‡ºæ–‡ä»¶å
        workflow_json = self._fill_output_names(workflow_json, context)
        
        # 4. è¡¥å…¨é»˜è®¤å‚æ•°
        workflow_json = self._fill_default_parameters(workflow_json)
        
        return workflow_json
    
    def _fill_prompts(
        self,
        workflow_json: Dict,
        user_request: UserRequest
    ) -> Dict:
        """å¡«å……æç¤ºè¯"""
        
        # æå–ç”¨æˆ·éœ€æ±‚ä¸­çš„å…³é”®æè¿°
        prompt = self._extract_prompt_from_request(user_request.raw_text)
        
        # æ‰¾åˆ°æ‰€æœ‰æ–‡æœ¬ç¼–ç èŠ‚ç‚¹
        for node_id, node in workflow_json.items():
            if node['class_type'] in ['CLIPTextEncode', 'CLIPTextEncodeSDXL']:
                # æ£€æŸ¥textå­—æ®µæ˜¯å¦ä¸ºç©ºæˆ–å ä½ç¬¦
                if 'text' in node['inputs']:
                    current_text = node['inputs']['text']
                    if not current_text or current_text == "" or current_text.startswith("{"):
                        node['inputs']['text'] = prompt
        
        return workflow_json
    
    def _fill_input_paths(
        self,
        workflow_json: Dict,
        input_file: str
    ) -> Dict:
        """å¡«å……è¾“å…¥æ–‡ä»¶è·¯å¾„"""
        
        # æ‰¾åˆ°æ‰€æœ‰åŠ è½½èŠ‚ç‚¹
        for node_id, node in workflow_json.items():
            if node['class_type'] in ['LoadImage', 'LoadVideo']:
                if 'image' in node['inputs']:
                    node['inputs']['image'] = input_file
                elif 'video' in node['inputs']:
                    node['inputs']['video'] = input_file
        
        return workflow_json
    
    def _fill_output_names(
        self,
        workflow_json: Dict,
        context: Optional[Dict]
    ) -> Dict:
        """å¡«å……è¾“å‡ºæ–‡ä»¶å"""
        
        output_prefix = context.get("output_prefix", "comfyui_output") if context else "comfyui_output"
        
        # æ‰¾åˆ°æ‰€æœ‰ä¿å­˜èŠ‚ç‚¹
        for node_id, node in workflow_json.items():
            if node['class_type'] in ['SaveImage', 'SaveVideo']:
                if 'filename_prefix' in node['inputs']:
                    node['inputs']['filename_prefix'] = output_prefix
        
        return workflow_json
    
    def _fill_default_parameters(self, workflow_json: Dict) -> Dict:
        """å¡«å……é»˜è®¤å‚æ•°"""
        
        # ä¸ºé‡‡æ ·å™¨è®¾ç½®é»˜è®¤å‚æ•°
        for node_id, node in workflow_json.items():
            if node['class_type'] == 'KSampler':
                # seedï¼ˆå¦‚æœä¸ºç©ºï¼Œä½¿ç”¨éšæœºï¼‰
                if 'seed' not in node['inputs'] or node['inputs']['seed'] == 0:
                    node['inputs']['seed'] = random.randint(0, 2**32 - 1)
                
                # stepsï¼ˆå¦‚æœä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤20ï¼‰
                if 'steps' not in node['inputs']:
                    node['inputs']['steps'] = 20
                
                # cfgï¼ˆå¦‚æœä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤7.0ï¼‰
                if 'cfg' not in node['inputs']:
                    node['inputs']['cfg'] = 7.0
        
        return workflow_json
    
    def _final_validation(self, workflow_json: Dict) -> Tuple[bool, List[str]]:
        """æœ€ç»ˆéªŒè¯"""
        
        errors = []
        
        # 1. æ£€æŸ¥æ‰€æœ‰å¿…éœ€å‚æ•°æ˜¯å¦å·²å¡«å……
        for node_id, node in workflow_json.items():
            node_type = node['class_type']
            required_params = self._get_required_parameters(node_type)
            
            for param in required_params:
                if param not in node['inputs'] or node['inputs'][param] is None:
                    errors.append(
                        f"èŠ‚ç‚¹{node_id}({node_type})ç¼ºå°‘å¿…éœ€å‚æ•°: {param}"
                    )
        
        # 2. å†æ¬¡æ£€æŸ¥DAGæœ‰æ•ˆæ€§
        if not is_valid_dag(workflow_json):
            errors.append("æœ€ç»ˆDAGç»“æ„æ— æ•ˆ")
        
        return len(errors) == 0, errors
```

---

## ğŸ”„ å®Œæ•´æµç¨‹ç¤ºä¾‹

```python
class ComfyUIWorkflowGenerator:
    """ComfyUIå·¥ä½œæµç”Ÿæˆç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config: Dict):
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.workflow_library = WorkflowLibrary(config['library_path'])
        self.need_decomposer = NeedDecomposer(config['llm_client'])
        self.need_matcher = NeedWorkflowMatcher(self.workflow_library)
        self.workflow_adapter = WorkflowAdapter()
        self.fragment_generator = FragmentGenerator(config['node_library'])
        self.workflow_validator = WorkflowValidator()
        self.workflow_synthesizer = WorkflowSynthesizer()
    
    def generate(
        self,
        user_request: UserRequest,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´ç”Ÿæˆæµç¨‹
        """
        
        # === é˜¶æ®µ1: éœ€æ±‚åŒ¹é… ===
        
        # 1.1 éœ€æ±‚åˆ†è§£
        decomposed = self.need_decomposer.decompose(user_request)
        atomic_needs = decomposed.atomic_needs
        
        # 1.2 & 1.3 æ„å›¾åŒ¹é… + ç¡®å®šåŒ¹é…/æœªåŒ¹é…
        match_results = self.need_matcher.match_all(atomic_needs)
        
        # === é˜¶æ®µ2: å·¥ä½œæµæ¡†æ¶çº§åˆ«é€‚é… ===
        
        # 2.1 æ‹†åˆ†-æ˜ å°„-æ‹¼æ¥
        framework = self.workflow_adapter.adapt(
            atomic_needs,
            match_results
        )
        
        # 2.2 ç”Ÿæˆç¼ºå¤±ç‰‡æ®µ
        for i, match in enumerate(match_results):
            if not match.matched:
                # ç”Ÿæˆç¼ºå¤±çš„ç‰‡æ®µ
                generated_fragment = self.fragment_generator.generate_fragment(
                    atomic_needs[i]
                )
                
                # æ’å…¥åˆ°æ¡†æ¶ä¸­
                framework.fragments.insert(i, generated_fragment)
        
        # é‡æ–°ç»„è£…ï¼ˆæ’å…¥äº†æ–°ç‰‡æ®µåï¼‰
        framework = self.workflow_adapter.fragment_assembler.assemble(
            framework.fragments,
            atomic_needs
        )
        
        # 2.3 å®Œæ•´æ€§æ£€æŸ¥
        is_valid, errors = self.workflow_validator.validate(framework)
        
        if not is_valid:
            # å°è¯•ä¿®å¤æˆ–æŠ¥å‘Šé”™è¯¯
            raise ValueError(f"å·¥ä½œæµæ¡†æ¶éªŒè¯å¤±è´¥: {errors}")
        
        # === é˜¶æ®µ3: å¯æ‰§è¡Œå·¥ä½œæµåˆæˆ ===
        
        # 3.1 è½¬æ¢ä¸ºJSON + å‚æ•°è¡¥å…¨
        workflow_json = self.workflow_synthesizer.synthesize(
            framework,
            user_request,
            context
        )
        
        return workflow_json

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # é…ç½®
    config = {
        'library_path': './data/workflow_library',
        'llm_client': OpenAIClient(api_key="..."),
        'node_library': load_node_library('./data/node_meta.json')
    }
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = ComfyUIWorkflowGenerator(config)
    
    # ç”¨æˆ·éœ€æ±‚
    request = UserRequest(
        raw_text="ç”Ÿæˆä¸€ä¸ªç²˜åœŸé£æ ¼çš„äººç‰©è‚–åƒï¼Œå¹¶è¿›è¡Œ4å€è¶…åˆ†"
    )
    
    # ç”Ÿæˆå·¥ä½œæµ
    workflow_json = generator.generate(request)
    
    # ä¿å­˜
    with open('output_workflow.json', 'w') as f:
        json.dump(workflow_json, f, indent=2)
    
    print("å·¥ä½œæµç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()
```

---

## ğŸ“… å®æ–½è®¡åˆ’ï¼ˆ8-10å‘¨ï¼‰

### Week 1-2: åŸºç¡€è®¾æ–½
- [ ] è®¾è®¡å’Œå®ç°æ•°æ®ç»“æ„ï¼ˆWorkflowEntry, AtomicCapabilityç­‰ï¼‰
- [ ] å»ºç«‹å·¥ä½œæµåº“å­˜å‚¨ç³»ç»Ÿ
- [ ] ä»å‰ä½œæå–æ‹¼æ¥ç®—æ³•
- [ ] å®Œå–„åŒå‘è§£æå™¨ï¼ˆJSON â†” Codeï¼‰

### Week 3-4: å·¥ä½œæµåº“æ„å»º
- [ ] çˆ¬å–ComfyBenchçš„20ä¸ªcurriculum workflows
- [ ] çˆ¬å–ç¤¾åŒºå·¥ä½œæµï¼ˆOpenArt, Civitaiï¼‰
- [ ] å®ç°è‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼ˆæ„å›¾æå–ã€èƒ½åŠ›æå–ï¼‰
- [ ] äººå·¥å®¡æ ¸å’Œè¡¥å……
- [ ] æ„å»ºå‘é‡ç´¢å¼•

### Week 5-6: éœ€æ±‚åŒ¹é…ï¼ˆé˜¶æ®µ1ï¼‰
- [ ] å®ç°NeedDecomposerï¼ˆéœ€æ±‚åˆ†è§£ï¼‰
- [ ] å®ç°IntentMatcherï¼ˆæ„å›¾åŒ¹é…ï¼‰
- [ ] å®ç°NeedWorkflowMatcherï¼ˆåŒ¹é…å†³ç­–ï¼‰
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•

### Week 7-8: å·¥ä½œæµé€‚é…ï¼ˆé˜¶æ®µ2æ ¸å¿ƒï¼‰
- [ ] å®ç°FragmentSplitterï¼ˆå·¥ä½œæµæ‹†åˆ†ï¼‰
- [ ] å®ç°ç‰‡æ®µ-éœ€æ±‚æ˜ å°„ç®—æ³•
- [ ] å®ç°FragmentAssemblerï¼ˆæ‹¼æ¥ç®—æ³•ï¼‰
- [ ] å®ç°FragmentGeneratorï¼ˆç¼ºå¤±ç‰‡æ®µç”Ÿæˆï¼‰
- [ ] å®ç°WorkflowValidatorï¼ˆéªŒè¯å™¨ï¼‰

### Week 9: å·¥ä½œæµåˆæˆï¼ˆé˜¶æ®µ3ï¼‰
- [ ] å®ç°WorkflowSynthesizerï¼ˆå‚æ•°è¡¥å…¨ï¼‰
- [ ] å®ç°æœ€ç»ˆéªŒè¯
- [ ] ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•

### Week 10: è¯„ä¼°å’Œä¼˜åŒ–
- [ ] åœ¨ComfyBenchä¸Šè¯„ä¼°
- [ ] Pass Rateç»Ÿè®¡
- [ ] ä¸ComfyAgentã€R1å¯¹æ¯”
- [ ] Bugä¿®å¤å’Œä¼˜åŒ–

---

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´ã€è¯¦ç»†ã€å¯æ‰§è¡Œçš„è®¾è®¡æ–¹æ¡ˆã€‚å…³é”®ç‚¹ï¼š

1. âœ… **ä¸¥æ ¼éµå¾ªä½ çš„ä¸‰é˜¶æ®µæ¡†æ¶**
2. âœ… **å¤ç”¨å‰ä½œçš„ä¸¥è°¨ç®—æ³•**ï¼ˆä¿è¯é«˜Pass Rateï¼‰
3. âœ… **èåˆComfyBenchå’Œç”Ÿæ€åŒ–æ€æƒ³**
4. âœ… **æ¯ä¸ªæ­¥éª¤éƒ½æœ‰å…·ä½“ç®—æ³•å’Œæ•°æ®ç»“æ„**
5. âœ… **å¯ç›´æ¥ç¼–ç å®ç°**

éœ€è¦æˆ‘å±•å¼€å“ªä¸ªå…·ä½“æ¨¡å—çš„å®ç°å—ï¼Ÿ
